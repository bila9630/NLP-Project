{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cca75f3",
   "metadata": {},
   "source": [
    "# Model Development\n",
    "We want to develop and compare different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "815501f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# Utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data science utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0410efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMMENT the line below if your system doesn't have wordnet\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f1eff9",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cce483b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffpost.com/entry/covid-boosters-...</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "      <td>Carla K. Johnson, AP</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffpost.com/entry/american-airlin...</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "      <td>Mary Papenfuss</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-tweets...</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "      <td>Elyse Wanshel</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-parent...</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "      <td>Caroline Bologna</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffpost.com/entry/amy-cooper-lose...</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "      <td>Nina Golgowski</td>\n",
       "      <td>2022-09-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  https://www.huffpost.com/entry/covid-boosters-...   \n",
       "1  https://www.huffpost.com/entry/american-airlin...   \n",
       "2  https://www.huffpost.com/entry/funniest-tweets...   \n",
       "3  https://www.huffpost.com/entry/funniest-parent...   \n",
       "4  https://www.huffpost.com/entry/amy-cooper-lose...   \n",
       "\n",
       "                                            headline   category  \\\n",
       "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
       "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
       "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
       "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
       "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
       "\n",
       "                                   short_description               authors  \\\n",
       "0  Health experts said it is too early to predict...  Carla K. Johnson, AP   \n",
       "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
       "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
       "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
       "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
       "\n",
       "        date  \n",
       "0 2022-09-23  \n",
       "1 2022-09-23  \n",
       "2 2022-09-23  \n",
       "3 2022-09-23  \n",
       "4 2022-09-22  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "df = pd.read_json(\"../data/News_Category_Dataset_v3.json\", orient = \"records\", lines = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d55bec14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COMEDY</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PARENTING</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category                                               text\n",
       "0  U.S. NEWS  Over 4 Million Americans Roll Up Sleeves For O...\n",
       "1  U.S. NEWS  American Airlines Flyer Charged, Banned For Li...\n",
       "2     COMEDY  23 Of The Funniest Tweets About Cats And Dogs ...\n",
       "3  PARENTING  The Funniest Tweets From Parents This Week (Se...\n",
       "4  U.S. NEWS  Woman Who Called Cops On Black Bird-Watcher Lo..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selected only relevant columns\n",
    "df_filtered = df[[\"headline\", \"category\", \"short_description\"]].copy()\n",
    "\n",
    "# Concatenate headline and short description together\n",
    "df_filtered.loc[:, \"text\"] = df_filtered[\"headline\"] + \" \" + df_filtered[\"short_description\"]\n",
    "df_filtered.drop([\"headline\",\"short_description\"],axis=1,inplace=True)\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92d38bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency in each category\n",
    "def count_category_frequency(df, column_name):\n",
    "    freq = df[column_name].value_counts()\n",
    "    print(f\"There are {len(freq)} categories\")\n",
    "    print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13ce8e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 42 categories\n",
      "POLITICS          35602\n",
      "WELLNESS          17945\n",
      "ENTERTAINMENT     17362\n",
      "TRAVEL             9900\n",
      "STYLE & BEAUTY     9814\n",
      "PARENTING          8791\n",
      "HEALTHY LIVING     6694\n",
      "QUEER VOICES       6347\n",
      "FOOD & DRINK       6340\n",
      "BUSINESS           5992\n",
      "COMEDY             5400\n",
      "SPORTS             5077\n",
      "BLACK VOICES       4583\n",
      "HOME & LIVING      4320\n",
      "PARENTS            3955\n",
      "THE WORLDPOST      3664\n",
      "WEDDINGS           3653\n",
      "WOMEN              3572\n",
      "CRIME              3562\n",
      "IMPACT             3484\n",
      "DIVORCE            3426\n",
      "WORLD NEWS         3299\n",
      "MEDIA              2944\n",
      "WEIRD NEWS         2777\n",
      "GREEN              2622\n",
      "WORLDPOST          2579\n",
      "RELIGION           2577\n",
      "STYLE              2254\n",
      "SCIENCE            2206\n",
      "TECH               2104\n",
      "TASTE              2096\n",
      "MONEY              1756\n",
      "ARTS               1509\n",
      "ENVIRONMENT        1444\n",
      "FIFTY              1401\n",
      "GOOD NEWS          1398\n",
      "U.S. NEWS          1377\n",
      "ARTS & CULTURE     1339\n",
      "COLLEGE            1144\n",
      "LATINO VOICES      1130\n",
      "CULTURE & ARTS     1074\n",
      "EDUCATION          1014\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "count_category_frequency(df_filtered, \"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc17929",
   "metadata": {},
   "source": [
    "There are many unique categories in the dataset that are not commonly found. These include categories such as \"Weird news,\" \"Green,\" and \"Fifty.\" Additionally, there is confusion about when a category should be classified as \"World news\" versus \"World post,\" and when a category should be classified as \"Money\" versus \"Business.\" To improve the applicability of the machine learning model to general datasets, **we will only retain categories that have more than 4000 articles**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d515bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_categories(df, threshold):\n",
    "    # Make a copy of the dataframe\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Create a new column \"count\" that stores the count of each category\n",
    "    data[\"count\"] = data.groupby(\"category\")[\"category\"].transform(\"count\")\n",
    "    # Keep only the rows where the \"count\" column is greater than the specified threshold\n",
    "    \n",
    "    data = data[data[\"count\"] > threshold]\n",
    "    \n",
    "    # Drop the \"count\" column\n",
    "    data.drop(columns=[\"count\"], inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10b29fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_over_4000 = filter_categories(df_filtered, 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80f9a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source for this func: https://www.kaggle.com/code/tarunchaubey/news-category-classification-machine-learning\n",
    "# since preprocessing always look the same, we will just utilize existing code instead of reinvent the wheel\n",
    "\n",
    "# preprocess text (removing stopwords and tokenizing)\n",
    "def process_text(text):\n",
    "    # convert text to lowercase, remove newlines and carriage returns, and strip leading/trailing whitespace\n",
    "    text = text.lower().replace('\\n',' ').replace('\\r','').strip()\n",
    "    \n",
    "    # replace multiple spaces with single space\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    \n",
    "    # remove non-alphanumeric characters and digits\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub(r'[0-9]','',text)\n",
    "    \n",
    "    # create set of english stopwords\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    \n",
    "    # tokenize text into words\n",
    "    word_tokens = word_tokenize(text)\n",
    "    \n",
    "    # if word not in stops_words, add word to filtered_sentence\n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w) \n",
    "    \n",
    "    text = \" \".join(filtered_sentence)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c664ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_over_4000[\"text\"] = df_over_4000[\"text\"].apply(lambda x:process_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee148d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_length = df_filtered[\"text\"].apply(len).sum()\n",
    "new_length = df_over_4000[\"text\"].apply(len).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5a47370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 36378921 characters\n",
      "After: 17948453 characters\n",
      "Total lost of: 18430468 characters\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before: {old_length} characters\")\n",
    "print(f\"After: {new_length} characters\")\n",
    "print(f\"Total lost of: {old_length - new_length} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7b3b7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COMEDY</td>\n",
       "      <td>funniest tweets cats dogs week sept dog dont u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PARENTING</td>\n",
       "      <td>funniest tweets parents week sept accidentally...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>maury wills basestealing shortstop dodgers die...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>golden globes returning nbc january year offai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>POLITICS</td>\n",
       "      <td>biden says us forces would defend taiwan china...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         category                                               text\n",
       "2          COMEDY  funniest tweets cats dogs week sept dog dont u...\n",
       "3       PARENTING  funniest tweets parents week sept accidentally...\n",
       "17         SPORTS  maury wills basestealing shortstop dodgers die...\n",
       "20  ENTERTAINMENT  golden globes returning nbc january year offai...\n",
       "21       POLITICS  biden says us forces would defend taiwan china..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_over_4000.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf3d8fef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14 categories\n",
      "POLITICS          35602\n",
      "WELLNESS          17945\n",
      "ENTERTAINMENT     17362\n",
      "TRAVEL             9900\n",
      "STYLE & BEAUTY     9814\n",
      "PARENTING          8791\n",
      "HEALTHY LIVING     6694\n",
      "QUEER VOICES       6347\n",
      "FOOD & DRINK       6340\n",
      "BUSINESS           5992\n",
      "COMEDY             5400\n",
      "SPORTS             5077\n",
      "BLACK VOICES       4583\n",
      "HOME & LIVING      4320\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "count_category_frequency(df_over_4000, \"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a949439b",
   "metadata": {},
   "source": [
    "The data is still imbalanced through. There are many different technique to counter this: \n",
    "- **Oversampling**: This technique involves duplicating instances of the underrepresented class(es) in the dataset to balance the class distribution. This can be done by randomly repeating instances of the underrepresented class(es) until the number of instances of each class is roughly equal.\n",
    "- **Undersampling**: This technique involves removing instances of the overrepresented class(es) in the dataset to balance the class distribution. This can be done by randomly removing instances of the overrepresented class(es) until the number of instances of each class is roughly equal.\n",
    "- **SMOTE**: (Synthetic Minority Over-sampling Technique) This is a more sophisticated oversampling technique that generates synthetic instances of the underrepresented class(es) by interpolating between existing instances of that class.\n",
    "- **Data augmentation**: This technique involves generating new instances of the underrepresented class(es) by applying data transformation techniques such as synonym replacement, random insertion, random deletion, or random swap to the existing instances of the underrepresented class(es).\n",
    "\n",
    "For our use case, **undersampling** is not suitable since we would lose a lot of valuable data in the categories \"politics\", \"wellness\" and \"entertainment\". **SMOTE** is not a suitable technique either because it operates in feature space, which means it generates synthetic data by interpolating between existing instances of the underrepresented class. However, in NLP problems, the feature space is large and high-dimensional, which makes it difficult for the KNN algorithm used by SMOTE to effectively identify similar instances. Additionally, the generated synthetic data may not be representative of real text data in the feature space, which may negatively impact the performance of the model. **Oversampling** can lead to overfitting, as the model becomes more sensitive to the duplicated instances of the minority class. Because of those reasons **we will use data augmentation with synonym replacement!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17322cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11 categories that have less than 10000 samples\n",
      "['TRAVEL', 'STYLE & BEAUTY', 'PARENTING', 'HEALTHY LIVING', 'QUEER VOICES', 'FOOD & DRINK', 'BUSINESS', 'COMEDY', 'SPORTS', 'BLACK VOICES', 'HOME & LIVING']\n"
     ]
    }
   ],
   "source": [
    "# get number of samples for each category\n",
    "class_counts = df_over_4000[\"category\"].value_counts()\n",
    "\n",
    "# threshold for sample\n",
    "threshold = 10000\n",
    "\n",
    "# get the list of categories that has less sample than threshold\n",
    "underrepresented_classes = class_counts[class_counts < threshold].index.tolist()\n",
    "\n",
    "print(f\"There are {len(underrepresented_classes)} categories that have less than {threshold} samples\")\n",
    "print(underrepresented_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "645e3e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_instance(text, label):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # error handling when text is not tokenizable\n",
    "    if len(tokens) == 0:\n",
    "        return text, label\n",
    "    \n",
    "    # Select a random token\n",
    "    random_token = random.choice(tokens)\n",
    "    \n",
    "    # Synonym dict\n",
    "    synonyms = wordnet.synsets(random_token)\n",
    "    \n",
    "    # when arent any synonyms for the token, then dont modify the text\n",
    "    if len(synonyms) == 0:\n",
    "        return text, label\n",
    "    else:\n",
    "        # Select a random synonym of the selected token\n",
    "        random_synonym = random.choice(synonyms).lemmas()[0].name()\n",
    "        # Replace the selected token with its synonym in the text\n",
    "        new_text = text.replace(random_token, random_synonym)\n",
    "        return new_text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3eabe1cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [10:25<00:00, 56.88s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the additional instances\n",
    "additional_instances = []\n",
    "\n",
    "# Iterate through all the underrepresented classes\n",
    "for label in tqdm(underrepresented_classes):\n",
    "    \n",
    "    # Determine the number of instances to generate for this class\n",
    "    num_to_generate = threshold - class_counts[label]\n",
    "    \n",
    "    # Generate the new instances\n",
    "    for i in range(num_to_generate):\n",
    "        # select random text from the underrepresented class\n",
    "        text = df_over_4000[df_over_4000[\"category\"] == label][\"text\"].sample(n=1).values[0]\n",
    "        # generate a new instance and label\n",
    "        new_text, new_label = generate_new_instance(text, label)\n",
    "        # append the new instance and label to the additional_instances list\n",
    "        additional_instances.append((new_text, new_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df50cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the additional instances to the original dataset\n",
    "df_aug = df_over_4000.append(pd.DataFrame(additional_instances, columns=[\"text\", \"category\"]), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42256f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14 categories\n",
      "POLITICS          35602\n",
      "WELLNESS          17945\n",
      "ENTERTAINMENT     17362\n",
      "COMEDY            10000\n",
      "PARENTING         10000\n",
      "SPORTS            10000\n",
      "BUSINESS          10000\n",
      "STYLE & BEAUTY    10000\n",
      "FOOD & DRINK      10000\n",
      "QUEER VOICES      10000\n",
      "HOME & LIVING     10000\n",
      "BLACK VOICES      10000\n",
      "TRAVEL            10000\n",
      "HEALTHY LIVING    10000\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "count_category_frequency(df_aug, \"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e40b887b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows previous 144167\n",
      "Number of rows after 180909\n",
      "In total we augmented 36742 rows\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows previous {len(df_over_4000)}\")\n",
    "print(f\"Number of rows after {len(df_aug)}\")\n",
    "print(f\"In total we augmented {len(df_aug) - len(df_over_4000)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fc4bbb",
   "metadata": {},
   "source": [
    "While inspecting the process, there are some disadvantages from data augmentation which could affect the model performance:\n",
    "- **grammatical errors**: ...make things worse people started pitying -> ...make things bad people started pitying\n",
    "- **nonsense**: rory mcilroy pulls olympics... -> rory mcilroy puff olympics...\n",
    "\n",
    "But those disadvantages are minimal so its fine for our use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "698b856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the line below to save the dataframe after preprocessing\n",
    "# df_aug.to_csv(\"df_aug.csv\", index=False, mode=\"wb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4f85c4",
   "metadata": {},
   "source": [
    "## Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3a32cf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da6468d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_aug[\"text\"]\n",
    "y = df_aug[\"category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe70480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2 ,random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24809803",
   "metadata": {},
   "source": [
    "### Naives Bayes\n",
    "First model we try is a Naives Bayer Classifier. It is based on the assumption of independence between features, which makes it a \"naive\" classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "86b47b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()), (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()), (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n",
    "nb = Pipeline([(\"vect\", CountVectorizer()),\n",
    "               (\"tfidf\", TfidfTransformer()),\n",
    "               (\"clf\", MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05669b9",
   "metadata": {},
   "source": [
    "- **The first step vect** uses the CountVectorizer class to convert the training data (X_train) into a sparse matrix of token counts. This step is also known as text vectorization. Text vectorization is the process of converting raw text data into a numerical format\n",
    "- **The second step tfidf** uses the TfidfTransformer class to transform the token counts into the Tf-idf representation of the data. Tf-idf (term frequency-inverse document frequency) is a measure of the importance of a word in a document, with respect to an entire corpus of documents.\n",
    "- **The final step clf**  uses the MultinomialNB class on the Tf-idf representation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8174c94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6089492012602952\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "  BLACK VOICES       0.92      0.22      0.36      1990\n",
      "      BUSINESS       0.94      0.21      0.34      2014\n",
      "        COMEDY       0.87      0.24      0.37      1971\n",
      " ENTERTAINMENT       0.61      0.75      0.67      3582\n",
      "  FOOD & DRINK       0.87      0.74      0.80      1995\n",
      "HEALTHY LIVING       0.95      0.04      0.07      2022\n",
      " HOME & LIVING       0.91      0.75      0.82      1991\n",
      "     PARENTING       0.84      0.35      0.49      1993\n",
      "      POLITICS       0.48      0.98      0.64      7000\n",
      "  QUEER VOICES       0.97      0.35      0.51      2003\n",
      "        SPORTS       0.91      0.56      0.69      2006\n",
      "STYLE & BEAUTY       0.89      0.66      0.76      2011\n",
      "        TRAVEL       0.89      0.56      0.69      2014\n",
      "      WELLNESS       0.45      0.87      0.59      3590\n",
      "\n",
      "      accuracy                           0.61     36182\n",
      "     macro avg       0.82      0.52      0.56     36182\n",
      "  weighted avg       0.75      0.61      0.58     36182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = nb.predict(X_test)\n",
    "print(\"accuracy %s\" % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd03aac",
   "metadata": {},
   "source": [
    "**Average accuracy is 0.61**! Since we have 14 categories, **the baseline would be 1 / 14 = 0.07**, when the model just randomly guesses! So the model itself is pretty good! Furthermore categories like Entertainment (0.67), Food & Drink (0.80) and Home & Living (0.82) seems easy to recognize. Let try to improve the model with hyperparameter tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "943acb11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect', CountVectorizer()),\n",
       "  ('tfidf', TfidfTransformer()),\n",
       "  ('clf', MultinomialNB())],\n",
       " 'verbose': False,\n",
       " 'vect': CountVectorizer(),\n",
       " 'tfidf': TfidfTransformer(),\n",
       " 'clf': MultinomialNB(),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': None,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__alpha': 1.0,\n",
       " 'clf__class_prior': None,\n",
       " 'clf__fit_prior': True}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display hyperparameters that we can change\n",
    "nb.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c2d6c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"vect__ngram_range\": [(1, 1), (1, 2)],\n",
    "              \"tfidf__use_idf\": (True, False),\n",
    "              \"clf__alpha\": (1e-2, 1e-3),\n",
    "}\n",
    "\n",
    "# GridSearchCV performs an exhaustive search over all possible combinations\n",
    "# This is much longer than RandomizedSearchCV\n",
    "gs_clf = GridSearchCV(nb, parameters, cv=5)\n",
    "gs_clf = gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e13a10a",
   "metadata": {},
   "source": [
    "- **vect__ngram_range**: Controls the range of n-grams that the vectorizer should consider\n",
    "- **tfidf__use_idf**: Controls whether or not the Tf-idf transformer should use the idf weighting scheme.\n",
    "- **clf__alpha**: Controls the regularization strength of the Naive Bayes classifier. Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d70e10d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8400585926703886\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "  BLACK VOICES       0.89      0.87      0.88      1990\n",
      "      BUSINESS       0.90      0.80      0.85      2014\n",
      "        COMEDY       0.86      0.80      0.83      1971\n",
      " ENTERTAINMENT       0.81      0.79      0.80      3582\n",
      "  FOOD & DRINK       0.89      0.91      0.90      1995\n",
      "HEALTHY LIVING       0.84      0.58      0.68      2022\n",
      " HOME & LIVING       0.92      0.95      0.94      1991\n",
      "     PARENTING       0.79      0.72      0.75      1993\n",
      "      POLITICS       0.85      0.93      0.89      7000\n",
      "  QUEER VOICES       0.91      0.84      0.87      2003\n",
      "        SPORTS       0.93      0.93      0.93      2006\n",
      "STYLE & BEAUTY       0.88      0.82      0.85      2011\n",
      "        TRAVEL       0.85      0.80      0.82      2014\n",
      "      WELLNESS       0.66      0.85      0.75      3590\n",
      "\n",
      "      accuracy                           0.84     36182\n",
      "     macro avg       0.86      0.83      0.84     36182\n",
      "  weighted avg       0.85      0.84      0.84     36182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = gs_clf.predict(X_test)\n",
    "print(\"accuracy %s\" % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d5589e",
   "metadata": {},
   "source": [
    "An average accuracy of 0.84 is pretty good! There is an accuracy of at least 0.75 for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8ac8b364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence The Funniest Tweets From Parents This Week\n",
      "Test sentence after preprocessing: funniest tweets parents week\n",
      "Predicted category: PARENTING\n"
     ]
    }
   ],
   "source": [
    "# Test the model by passing a chosen sentence\n",
    "test_sentence = \"The Funniest Tweets From Parents This Week\"\n",
    "test_sentence_process = process_text(test_sentence)\n",
    "category_pred = gs_clf.predict([test_sentence_process])\n",
    "\n",
    "print(\"Original sentence\", test_sentence)\n",
    "print(\"Test sentence after preprocessing:\", test_sentence_process)\n",
    "print(\"Predicted category:\", category_pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783a857f",
   "metadata": {},
   "source": [
    "The sentence wasnt randomly chosen. It could belong to the category parenting but **it would fit much better to comedy**. The model probably see the word parent and automatically assign it to \"parenting\" category. This is a weakness of Naives Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a42c2118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code below to export the model\n",
    "# with open(\"naiveBayes.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(gs_clf.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c5244c",
   "metadata": {},
   "source": [
    "### Linear SVM\n",
    "Next linear model we will try is **Linear Support Vector Machine** . Linear Support Vector Machine (SVM) is a model that finds the best boundary to separate different classes in the feature space, by maximizing the distance between the boundary and the closest data points from each class. The SGDClassifier is a linear classifier that can also be used as a linear SVM by setting the loss parameter to \"hinge\" and the penalty parameter to \"l2\" and it learns from training data incrementally using stochastic gradient descent optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "975d64c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()), (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 SGDClassifier(alpha=0.001, max_iter=5, random_state=42,\n",
       "                               tol=None))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()), (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 SGDClassifier(alpha=0.001, max_iter=5, random_state=42,\n",
       "                               tol=None))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(alpha=0.001, max_iter=5, random_state=42, tol=None)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf',\n",
       "                 SGDClassifier(alpha=0.001, max_iter=5, random_state=42,\n",
       "                               tol=None))])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n",
    "sgd = Pipeline([(\"vect\", CountVectorizer()),\n",
    "                (\"tfidf\", TfidfTransformer()),\n",
    "                (\"clf\", SGDClassifier(loss=\"hinge\", penalty=\"l2\",alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ae33c17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6865844895251783\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "  BLACK VOICES       0.71      0.47      0.56      1990\n",
      "      BUSINESS       0.76      0.48      0.59      2014\n",
      "        COMEDY       0.68      0.37      0.48      1971\n",
      " ENTERTAINMENT       0.73      0.60      0.66      3582\n",
      "  FOOD & DRINK       0.72      0.83      0.77      1995\n",
      "HEALTHY LIVING       0.69      0.12      0.20      2022\n",
      " HOME & LIVING       0.73      0.82      0.77      1991\n",
      "     PARENTING       0.69      0.68      0.68      1993\n",
      "      POLITICS       0.62      0.94      0.75      7000\n",
      "  QUEER VOICES       0.81      0.72      0.77      2003\n",
      "        SPORTS       0.78      0.77      0.77      2006\n",
      "STYLE & BEAUTY       0.71      0.79      0.75      2011\n",
      "        TRAVEL       0.77      0.72      0.74      2014\n",
      "      WELLNESS       0.62      0.72      0.66      3590\n",
      "\n",
      "      accuracy                           0.69     36182\n",
      "     macro avg       0.72      0.64      0.65     36182\n",
      "  weighted avg       0.70      0.69      0.67     36182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = sgd.predict(X_test)\n",
    "\n",
    "print(\"accuracy %s\" % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e68edc",
   "metadata": {},
   "source": [
    "**Average score is 0.69** which is 8% better than Naive Bayes without hyperparameter tuning. Let apply hyperparameter tuning to increase the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9df7d809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect', CountVectorizer()),\n",
       "  ('tfidf', TfidfTransformer()),\n",
       "  ('clf', SGDClassifier(alpha=0.001, max_iter=5, random_state=42, tol=None))],\n",
       " 'verbose': False,\n",
       " 'vect': CountVectorizer(),\n",
       " 'tfidf': TfidfTransformer(),\n",
       " 'clf': SGDClassifier(alpha=0.001, max_iter=5, random_state=42, tol=None),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': None,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__alpha': 0.001,\n",
       " 'clf__average': False,\n",
       " 'clf__class_weight': None,\n",
       " 'clf__early_stopping': False,\n",
       " 'clf__epsilon': 0.1,\n",
       " 'clf__eta0': 0.0,\n",
       " 'clf__fit_intercept': True,\n",
       " 'clf__l1_ratio': 0.15,\n",
       " 'clf__learning_rate': 'optimal',\n",
       " 'clf__loss': 'hinge',\n",
       " 'clf__max_iter': 5,\n",
       " 'clf__n_iter_no_change': 5,\n",
       " 'clf__n_jobs': None,\n",
       " 'clf__penalty': 'l2',\n",
       " 'clf__power_t': 0.5,\n",
       " 'clf__random_state': 42,\n",
       " 'clf__shuffle': True,\n",
       " 'clf__tol': None,\n",
       " 'clf__validation_fraction': 0.1,\n",
       " 'clf__verbose': 0,\n",
       " 'clf__warm_start': False}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display hyperparameters that we can change\n",
    "sgd.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1f339e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()),\n",
       "                                       (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                                       (&#x27;clf&#x27;,\n",
       "                                        SGDClassifier(alpha=0.001, max_iter=5,\n",
       "                                                      random_state=42,\n",
       "                                                      tol=None))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;clf__alpha&#x27;: [0.001, 0.01, 0.1],\n",
       "                         &#x27;clf__max_iter&#x27;: [5, 10, 15],\n",
       "                         &#x27;clf__tol&#x27;: [None, 0.001, 0.0001]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()),\n",
       "                                       (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                                       (&#x27;clf&#x27;,\n",
       "                                        SGDClassifier(alpha=0.001, max_iter=5,\n",
       "                                                      random_state=42,\n",
       "                                                      tol=None))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;clf__alpha&#x27;: [0.001, 0.01, 0.1],\n",
       "                         &#x27;clf__max_iter&#x27;: [5, 10, 15],\n",
       "                         &#x27;clf__tol&#x27;: [None, 0.001, 0.0001]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()), (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 SGDClassifier(alpha=0.001, max_iter=5, random_state=42,\n",
       "                               tol=None))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(alpha=0.001, max_iter=5, random_state=42, tol=None)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect', CountVectorizer()),\n",
       "                                       ('tfidf', TfidfTransformer()),\n",
       "                                       ('clf',\n",
       "                                        SGDClassifier(alpha=0.001, max_iter=5,\n",
       "                                                      random_state=42,\n",
       "                                                      tol=None))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'clf__alpha': [0.001, 0.01, 0.1],\n",
       "                         'clf__max_iter': [5, 10, 15],\n",
       "                         'clf__tol': [None, 0.001, 0.0001]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the parameter grid for the SGDClassifier\n",
    "param_grid = {'clf__alpha': [0.001, 0.01, 0.1],\n",
    "              'clf__max_iter': [5, 10, 15],\n",
    "              'clf__tol': [None, 1e-3, 1e-4]\n",
    "             }\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "gs_sgd = GridSearchCV(sgd, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "gs_sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95afeb4a",
   "metadata": {},
   "source": [
    "- **clf_alpha**: Controls the regularization strength. Regularization is a technique that helps to prevent overfitting by adding a penalty term to the loss function that the model is trying to minimize. The alpha parameter determines the strength of this penalty term. A smaller alpha means a stronger regularization, and a larger alpha means a weaker regularization. By setting alpha to different values, you can control the trade-off between fitting the training data well and preventing overfitting.\n",
    "- **clf_max_iter**: Maximum number of iteration before the model stops.\n",
    "- **clf_tol**: is the tolerance for stopping criterion. It is used to control the stopping criteria of the optimizer. When the loss or score is not improving by at least tol for n_iter_no_change consecutive iterations, the training process is stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b9be10ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best set of parameters:  {'clf__alpha': 0.001, 'clf__max_iter': 10, 'clf__tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best set of parameters: \", gs_sgd.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dd3d4052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6859764523796363\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "  BLACK VOICES       0.71      0.48      0.57      1990\n",
      "      BUSINESS       0.76      0.48      0.59      2014\n",
      "        COMEDY       0.69      0.37      0.48      1971\n",
      " ENTERTAINMENT       0.74      0.59      0.66      3582\n",
      "  FOOD & DRINK       0.72      0.83      0.77      1995\n",
      "HEALTHY LIVING       0.68      0.12      0.20      2022\n",
      " HOME & LIVING       0.72      0.82      0.77      1991\n",
      "     PARENTING       0.69      0.68      0.68      1993\n",
      "      POLITICS       0.61      0.94      0.74      7000\n",
      "  QUEER VOICES       0.82      0.72      0.77      2003\n",
      "        SPORTS       0.78      0.77      0.77      2006\n",
      "STYLE & BEAUTY       0.72      0.79      0.75      2011\n",
      "        TRAVEL       0.77      0.71      0.74      2014\n",
      "      WELLNESS       0.61      0.73      0.66      3590\n",
      "\n",
      "      accuracy                           0.69     36182\n",
      "     macro avg       0.72      0.64      0.65     36182\n",
      "  weighted avg       0.70      0.69      0.67     36182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = gs_sgd.predict(X_test)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed440da",
   "metadata": {},
   "source": [
    "**No improvement at all**! The default parameters are already the best. The only improvement is on software engineering site and the model is only 13mb big. Naive bayes before is 342mb big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e740d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code below to export the model\n",
    "# with open(\"linearSVM.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(gs_sgd.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c5466f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence The Funniest Tweets From Parents This Week\n",
      "Test sentence after preprocessing: funniest tweets parents week\n",
      "Predicted category: PARENTING\n"
     ]
    }
   ],
   "source": [
    "# Let test the model\n",
    "# Test the model by passing a chosen sentence\n",
    "test_sentence = \"The Funniest Tweets From Parents This Week\"\n",
    "test_sentence_process = process_text(test_sentence)\n",
    "category_pred = gs_sgd.predict([test_sentence_process])\n",
    "\n",
    "print(\"Original sentence\", test_sentence)\n",
    "print(\"Test sentence after preprocessing:\", test_sentence_process)\n",
    "print(\"Predicted category:\", category_pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173fa869",
   "metadata": {},
   "source": [
    "The model didn't make an improvement either"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a3f120",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "The first model we will build is a **sequential model with a fully connected neural network in combination with BOW (Bag of Words)**. With BOW a text document gets converted into a numerical vector, where each dimension of the vector represents a word (or n-gram) from the vocabulary and the value in the dimension represents the frequency of that word in the document. The BOW model generates a vocabulary from the text corpus and then represent each document as a vector of the word counts. The BOW model is simple and easy to implement, but **it does not take into account the order of words in the text**, which may be important for some tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "352dcc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0951af9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COMEDY</td>\n",
       "      <td>funniest tweets cats dogs week sept dog dont u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PARENTING</td>\n",
       "      <td>funniest tweets parents week sept accidentally...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>maury wills basestealing shortstop dodgers die...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>golden globes returning nbc january year offai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POLITICS</td>\n",
       "      <td>biden says us forces would defend taiwan china...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0         COMEDY  funniest tweets cats dogs week sept dog dont u...\n",
       "1      PARENTING  funniest tweets parents week sept accidentally...\n",
       "2         SPORTS  maury wills basestealing shortstop dodgers die...\n",
       "3  ENTERTAINMENT  golden globes returning nbc january year offai...\n",
       "4       POLITICS  biden says us forces would defend taiwan china..."
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "72a6fb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of df: 180909\n"
     ]
    }
   ],
   "source": [
    "print(\"Total length of df:\", len(df_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b3f155cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144727"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use 80% of the dataset for training\n",
    "train_size = int(len(df_aug) * 0.8)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0fb4fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting df into train and test set\n",
    "train_text = df_aug[\"text\"][:train_size]\n",
    "train_category = df_aug[\"category\"][:train_size]\n",
    "\n",
    "test_text = df_aug[\"text\"][train_size:]\n",
    "test_category = df_aug[\"category\"][train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff044953",
   "metadata": {},
   "source": [
    "### NN with BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "22d9e37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum number of words to be included in the vocabulary\n",
    "max_words = 1000\n",
    "# Initialize the tokenizer\n",
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "# Fit the tokenizer only on the train text to create the vocabulary\n",
    "tokenize.fit_on_texts(train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0637169",
   "metadata": {},
   "source": [
    "- **max_words** sets the maximum number of words to be included in the vocabulary used for tokenization. It has a big impact onto the data! If it is set too low, the model might not be able to learn enough about the underlying patterns in the data, as it would have an incomplete vocabulary.  On the other hand, if num_words is set too high, the model will have to learn from a larger vocabulary and might be more prone to overfitting, since it would be memorizing noise from the data rather than generalizing.\n",
    "- **char_level** sets to false means that the tokenizer will split the text into words and create a vocabulary of all the unique words. Each word will be represented as a single token, and the tokenizer will take into account the word-level information. If it is set to true, the tokenizer will split the text into individual characters and create a vocabulary of all the unique characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "abf87a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train text into numerical feature vectors using the vocabulary created earlier\n",
    "x_train = tokenize.texts_to_matrix(train_text)\n",
    "x_test = tokenize.texts_to_matrix(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "bae377f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the label encoder \n",
    "# convert categorical variables (strings) into numerical variables (integers)\n",
    "encoder = LabelEncoder()\n",
    "# Fit the encoder on the train category to learn the mapping from the categories to integer labels\n",
    "encoder.fit(train_category)\n",
    "\n",
    "# Transform the category into integer labels using the mapping learned earlier\n",
    "y_train = encoder.transform(train_category)\n",
    "y_test = encoder.transform(test_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "81f21e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of classes by finding the maximum label value in the train set and adding 1\n",
    "# why? Current data look like: 1=\"Comedy\", 2=\"POLITIC\"\n",
    "num_classes = np.max(y_train) + 1\n",
    "\n",
    "# Data looks like [0 0 1] after one hot encoding\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "91bd46c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n",
    "# Build the model\n",
    "model_nn = Sequential()\n",
    "model_nn.add(Dense(512, input_shape=(max_words,)))\n",
    "model_nn.add(Activation(\"relu\"))\n",
    "model_nn.add(Dropout(0.5))\n",
    "model_nn.add(Dense(num_classes))\n",
    "model_nn.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bd60fa",
   "metadata": {},
   "source": [
    "What does the dropout layer do? The Dropout layer is a regularization technique for reducing overfitting in neural networks. It works by randomly \"dropping out\" or setting to zero a certain percentage of the neurons (specified by the dropout rate) during the training process. This means that these neurons will not be updated or participate in the forward or backward pass during training. By doing this, the model is forced to learn multiple independent representations of the data, rather than relying too heavily on any one neuron. This in turn reduces the chance of the model overfitting to the training data, and improves its generalization performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ab3f6229",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae8b1b8",
   "metadata": {},
   "source": [
    "- **categorical crossentropy** is used as loss function. Loss function is used to measure the model's performance and categorical crossentropy computes the cross-entropy loss between true labels and predicted labels.\n",
    "- **adam** is the optimizer and it adapts the learning rate for each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b589cd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples per gradient update\n",
    "batch_size = 32\n",
    "# Number of times the model will cycle through the data\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4f8225bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "4071/4071 [==============================] - 12s 3ms/step - loss: 1.3160 - accuracy: 0.6006 - val_loss: 1.2153 - val_accuracy: 0.6545\n",
      "Epoch 2/4\n",
      "4071/4071 [==============================] - 12s 3ms/step - loss: 1.1544 - accuracy: 0.6424 - val_loss: 1.2457 - val_accuracy: 0.6451\n",
      "Epoch 3/4\n",
      "4071/4071 [==============================] - 12s 3ms/step - loss: 1.0942 - accuracy: 0.6583 - val_loss: 1.2387 - val_accuracy: 0.6459\n",
      "Epoch 4/4\n",
      "4071/4071 [==============================] - 12s 3ms/step - loss: 1.0398 - accuracy: 0.6729 - val_loss: 1.2120 - val_accuracy: 0.6538\n"
     ]
    }
   ],
   "source": [
    "history = model_nn.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0076921b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1131/1131 [==============================] - 2s 1ms/step - loss: 1.4418 - accuracy: 0.4984\n",
      "Test accuracy: 0.49842461943626404\n"
     ]
    }
   ],
   "source": [
    "score = model_nn.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf2dbc0",
   "metadata": {},
   "source": [
    "Accuracy of around 50% is **in comparison to Naives Bayes pretty bad**. It's probably because of batch_size and epochs are too small. Max_words with 1000 is also too small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5f446331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02625877 0.01205577 0.16106945 0.14356118 0.00325501 0.04972285\n",
      "  0.00556945 0.4435004  0.05745911 0.01528602 0.02839412 0.03697094\n",
      "  0.00397735 0.01291951]]\n"
     ]
    }
   ],
   "source": [
    "# Let test the model\n",
    "\n",
    "# Tokenize the example text\n",
    "example_text = tokenize.texts_to_matrix([\"The Funniest Tweets From Parents This Week\"])\n",
    "\n",
    "# Make a prediction using the model\n",
    "predictions = model_nn.predict(example_text)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2b3f771a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARENTING 0.4435004\n"
     ]
    }
   ],
   "source": [
    "# Get the index of the class with the highest probability\n",
    "predicted_class_index = np.argmax(predictions)\n",
    "\n",
    "# Get the corresponding class label from the label encoder\n",
    "predicted_class_label = encoder.classes_[predicted_class_index]\n",
    "\n",
    "# Get the maximum probability\n",
    "predicted_class_prob = predictions[0][predicted_class_index]\n",
    "\n",
    "# Print the predicted class label and its probability\n",
    "print(predicted_class_label, predicted_class_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bfa900",
   "metadata": {},
   "source": [
    "At least the model gives the sample output like naive bayes and svm on the example sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "355ce9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the code below to export the model, tokenizer and encoder\n",
    "\n",
    "# model_nn.save(\"model_nn.h5\")\n",
    "# with open(\"tokenizer_nn.pickle\", \"wb\") as handle:\n",
    "#     pickle.dump(tokenize, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open(\"encoder_nn.pickle\", \"wb\") as handle:\n",
    "#     pickle.dump(encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bb95ee",
   "metadata": {},
   "source": [
    "### CNN with Embedding\n",
    "The next model we will develop is a **CNN**. One of the key features of CNNs is that they have a hidden vector, which acts as a short-term memory, allowing them to consider the sequence of words in a sentence. However, this advantage comes at a cost of slower sequential processing compared to other methods. The code in the beginning with Tokenizing is almost the same like with but I will copy it here again, so we can run each model development independently from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "c370bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import SimpleRNN, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "7163a680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144727"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use 80% of the dataset for training\n",
    "train_size = int(len(df_aug) * 0.8)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "58914c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting df into train and test set\n",
    "train_text = df_aug[\"text\"][:train_size]\n",
    "train_category = df_aug[\"category\"][:train_size]\n",
    "\n",
    "test_text = df_aug[\"text\"][train_size:]\n",
    "test_category = df_aug[\"category\"][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "44eb2aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will increase this from 1000 -> 20000\n",
    "max_words = 20000\n",
    "# Initialize the tokenizer\n",
    "tokenize_cnn = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "\n",
    "# Fit the tokenizer only on the train text to create the vocabulary\n",
    "tokenize_cnn.fit_on_texts(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "29d58807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required for Embedding\n",
    "maxlen = 110"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bdaba4",
   "metadata": {},
   "source": [
    "Why do we need to define maxlen?\n",
    "<br>maxlen is used to define the maximum length of the input sequences that will be passed to the Embedding layer. The Embedding layer expects a fixed-length input, and maxlen specifies the maximum length of the input sequences.\n",
    "<br>If the input sequence is shorter than maxlen, it will be padded with zeros to reach the specified length. If it is longer than maxlen, it will be truncated to maxlen words.\n",
    "<br>This ensures that all input sequences passed to the Embedding layer have the same length and can be used as input to the model.\n",
    "This is important because, in the given code, the Embedding layer is the first layer of the model and it expects a fixed-length input, so it is necessary to define the maxlen so that the Embedding layer can correctly handle the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "09c25ea8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# turn words into a list of sequences, where each sequence\n",
    "# is a list of integers representing the words in the text\n",
    "train_text_seq = tokenize_cnn.texts_to_sequences(train_text)\n",
    "# pad the sequences to a fixed length of 110\n",
    "train_text_padseq = pad_sequences(train_text_seq, maxlen=maxlen)\n",
    "\n",
    "# same for test data\n",
    "test_text_seq = tokenize_cnn.texts_to_sequences(test_text)\n",
    "test_text_padseq = pad_sequences(test_text_seq, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca80af2",
   "metadata": {},
   "source": [
    "Why do we need pad_sequences? pad_sequences ensures that all sequences have the same length and can be used as input to the CNN model. It also helps to handle variable length of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "690ebbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the label encoder \n",
    "# convert categorical variables (strings) into numerical variables (integers)\n",
    "encoder = LabelEncoder()\n",
    "# Fit the encoder on the train category to learn the mapping from the categories to integer labels\n",
    "encoder.fit(train_category)\n",
    "\n",
    "# Transform the category into integer labels using the mapping learned earlier\n",
    "y_train = encoder.transform(train_category)\n",
    "y_test = encoder.transform(test_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "003330c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of classes by finding the maximum label value in the train set and adding 1\n",
    "# why? Current data look like: 1=\"Comedy\", 2=\"POLITIC\"\n",
    "num_classes = np.max(y_train) + 1\n",
    "\n",
    "# Data looks like [0 0 1] after one hot encoding\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "2a8c5506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89936"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete later\n",
    "total_words = len(tokenize_cnn.word_index)\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "8d39eccb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_12 (Embedding)    (None, 110, 50)           1000000   \n",
      "                                                                 \n",
      " bidirectional_22 (Bidirecti  (None, 110, 128)         14720     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_23 (Bidirecti  (None, 110, 128)         24704     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " simple_rnn_35 (SimpleRNN)   (None, 32)                5152      \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 14)                462       \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 14)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,045,038\n",
      "Trainable params: 1,045,038\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# inspired by: https://www.kaggle.com/code/avikumart/nlp-news-articles-classif-wordembeddings-rnn\n",
    "\n",
    "# basline model using embedding layers and simpleRNN\n",
    "model_cnn = Sequential()\n",
    "# 50 represents the number of dimensions in the embedding space.\n",
    "# This means that each word in the vocabulary will be represented by a vector of 50 numbers\n",
    "model_cnn.add(Embedding(max_words, 50, input_length=maxlen))\n",
    "model_cnn.add(Bidirectional(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.20, activation=\"tanh\", return_sequences=True)))\n",
    "model_cnn.add(Bidirectional(SimpleRNN(64, dropout=0.3, recurrent_dropout=0.30, activation=\"tanh\", return_sequences=True)))\n",
    "model_cnn.add(SimpleRNN(32, activation=\"tanh\"))\n",
    "model_cnn.add(Dropout(0.4))\n",
    "model_cnn.add(Dense(num_classes))\n",
    "model_cnn.add(Activation(\"softmax\"))\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "f40d756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "f158702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples per gradient update\n",
    "# larger batch size -> faster training -> overfitting\n",
    "batch_size = 64\n",
    "# Number of times the model will cycle through the data\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "ef0404c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1810/1810 [==============================] - 383s 210ms/step - loss: 2.0961 - accuracy: 0.3623 - val_loss: 2.2464 - val_accuracy: 0.2494\n",
      "Epoch 2/4\n",
      "1810/1810 [==============================] - 379s 210ms/step - loss: 1.6415 - accuracy: 0.4908 - val_loss: 1.8901 - val_accuracy: 0.4020\n",
      "Epoch 3/4\n",
      "1810/1810 [==============================] - 376s 208ms/step - loss: 1.4393 - accuracy: 0.5544 - val_loss: 1.7075 - val_accuracy: 0.4968\n",
      "Epoch 4/4\n",
      "1810/1810 [==============================] - 368s 203ms/step - loss: 1.3237 - accuracy: 0.5879 - val_loss: 1.6945 - val_accuracy: 0.5156\n"
     ]
    }
   ],
   "source": [
    "# fit model to the data\n",
    "history = model_cnn.fit(train_text_padseq, y_train, \n",
    "                        batch_size=batch_size, \n",
    "                        epochs=epochs, \n",
    "                        validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "f830eedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566/566 [==============================] - 18s 33ms/step - loss: 2.1297 - accuracy: 0.2380\n",
      "Test accuracy: 0.2380465418100357\n"
     ]
    }
   ],
   "source": [
    "score = model_cnn.evaluate(test_text_padseq, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf64e20",
   "metadata": {},
   "source": [
    "**Accuracy of 0.23** is not good at all. This is pretty like guessing. Train accuracy of 0.6 and test accuracy of 0.23 means there is an overfitting. Dropout is already pretty high but we could increase it even more to prevent overfitting. Other options are decrease batch size and increase epochs. Also we could use LSTM or GRU but we will need to design a whole new architecture then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "120b5521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: HEALTHY LIVING\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "sentence = \"The Funniest Tweets From Parents This Week\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "sentence_seq = tokenize_cnn.texts_to_sequences([sentence])\n",
    "\n",
    "# Pad the sentence to the same length as the training data\n",
    "sentence_padseq = pad_sequences(sentence_seq, maxlen=maxlen)\n",
    "\n",
    "# Make a prediction using the model\n",
    "predictions = model_cnn.predict(sentence_padseq)\n",
    "\n",
    "# Get the class with the highest probability\n",
    "predicted_class = np.argmax(predictions)\n",
    "\n",
    "# Convert the integer label back to the original category\n",
    "predicted_category = encoder.inverse_transform([predicted_class])\n",
    "\n",
    "# Print the result\n",
    "print(\"Predicted category:\", predicted_category[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "39215f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the code below to export the model, tokenizer and encoder\n",
    "# model_cnn.save(\"model_cnn.h5\")\n",
    "# with open(\"tokenizer_cnn.pickle\", \"wb\") as handle:\n",
    "#     pickle.dump(tokenize, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open(\"encoder_cnn.pickle\", \"wb\") as handle:\n",
    "#     pickle.dump(encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5fa9bb2a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Overall our use case performs better with a simpler model like Naive Bayes compared to a deep learning model like CNN. The poor performance of the CNN could be due to overfitting, which can be addressed by adjusting parameters such as the dropout rate. Futhermore we could optimise our cnn model by increasing max_len. Max_len of 110 is pretty small for our use case. For NN we could increase the max_word. 1000 was probably too small it probably leads to a loss of important information and result in less accurate predictions. Additionally, the data augmentation we performed in the beginning may not have had a positive impact on the model's performance and it may be more effective to train the model directly on the original dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (tags/v3.9.6:db3ff76, Jun 28 2021, 15:26:21) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "941331122c4c5b0e9a76ba712854f74fda5ba1e38e6c463c3ac8fc3ab668796c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
